---
phase: 10-ai-fallback-fixes
plan: 02
type: execute
wave: 2
depends_on: ["10-01"]
files_modified: [.planning/phases/10-ai-fallback-fixes/10-VERIFICATION.md, .planning/phases/10-ai-fallback-fixes/10-CONTEXT.md]
autonomous: true

must_haves:
  truths:
    - "Verification steps exist to ensure AI fallback works in practice"
    - "Relevant tests run and pass (footer_extractor, ai client) so AI path is not regressed"
  artifacts:
    - path: ".planning/phases/10-ai-fallback-fixes/10-VERIFICATION.md"
      provides: "Steps to verify AI fallback end-to-end"
  key_links:
    - from: "10-VERIFICATION.md"
      to: "10-CONTEXT.md"
      via: "References doc for config and trigger"
---

<objective>
Add verification steps and run existing tests so we can ensure AI fallback works in practice. Depends on 10-01 (documentation exists).

Purpose: ROADMAP Phase 10 asks to "säkerställa att AI fallback fungerar bra i praktiken". This plan adds a verification checklist and confirms that the pipeline and AI-related tests still pass.

Output: `.planning/phases/10-ai-fallback-fixes/10-VERIFICATION.md` with concrete verification steps; 10-CONTEXT.md updated with a short "Verification" section that points to 10-VERIFICATION.md; confirmation that `pytest tests/test_footer_extractor.py tests/test_ai_client.py` passes.
</objective>

<execution_context>
@./.claude/get-shit-done/workflows/execute-plan.md
@./.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/phases/10-ai-fallback-fixes/10-CONTEXT.md
@.planning/ROADMAP.md
@src/pipeline/footer_extractor.py
@tests/test_footer_extractor.py
@tests/test_ai_client.py
</context>

<tasks>

<task type="auto">
  <name>Task 1: Create 10-VERIFICATION.md with verification steps</name>
  <files>.planning/phases/10-ai-fallback-fixes/10-VERIFICATION.md</files>
  <action>
Create a short verification document that describes how to confirm AI fallback works.

**Content:**

1. **Prerequisites**
   - AI enabled (via UI “AI-inställningar” or env `AI_ENABLED=true`).
   - API key set (`AI_KEY` or via UI).
   - Provider/model as in 10-CONTEXT.md.

2. **Automated checks**
   - Run: `pytest tests/test_footer_extractor.py tests/test_ai_client.py -v`
   - These tests today disable AI (e.g. monkeypatch `get_ai_enabled` / calibration) so they test heuristic path. Passing them ensures the footer extractor and AI client are not broken. Document this: “Unit tests disable AI for stability; they guard the extraction pipeline. AI-on behaviour is verified manually or via the steps below.”

3. **Manual verification**
   - Run the engine on one PDF that typically yields low heuristic confidence or few candidates (e.g. unusual layout, scan).
   - Use CLI or GUI with AI enabled and API key set.
   - Check logs for lines like “AI fallback …” (e.g. “AI fallback succeeded” or “AI fallback used: no heuristic candidates”).
   - Confirm the exported total matches expectations (and, if possible, that it improved vs heuristic-only).

4. **Optional**
   - If a test PDF or fixture exists that is known to trigger AI when enabled, mention it here for quick reuse.

**Acceptance:**
- 10-VERIFICATION.md exists and lists prerequisites, automated checks, and manual steps.
</action>
  <verify>File exists; manual steps are concrete (what to run, what to look for in logs).</verify>
  <done>10-VERIFICATION.md exists with prerequisites, automated checks, and manual verification steps.</done>
</task>

<task type="auto">
  <name>Task 2: Point 10-CONTEXT.md verification section to 10-VERIFICATION.md</name>
  <files>.planning/phases/10-ai-fallback-fixes/10-CONTEXT.md</files>
  <action>
In 10-CONTEXT.md, ensure the brief “Verification” subsection (or equivalent from 10-01) explicitly points to `10-VERIFICATION.md` for full steps. Add a line such as: “For full verification steps (automated tests, manual run, log checks), see 10-VERIFICATION.md.”

If 10-01 already included a one-sentence pointer, replace or extend it to reference 10-VERIFICATION.md.
</action>
  <verify>10-CONTEXT.md contains a clear reference to 10-VERIFICATION.md.</verify>
  <done>Documentation links verification details to 10-VERIFICATION.md.</done>
</task>

<task type="auto">
  <name>Task 3: Run tests and record result</name>
  <files>.planning/phases/10-ai-fallback-fixes/10-02-SUMMARY.md</files>
  <action>
Run `pytest tests/test_footer_extractor.py tests/test_ai_client.py -v` (or the project’s standard test command for these). In 10-02-SUMMARY.md, note that the run was performed and whether all tests passed. If any test failed, note the failure; the plan is still “done” once verification steps are documented and the run is recorded (fixing failing tests may be a separate follow-up).
</action>
  <verify>Test command was run; result is mentioned in the summary.</verify>
  <done>Relevant tests were run; pass/fail is recorded in 10-02-SUMMARY.</done>
</task>

</tasks>

<verification>
- 10-VERIFICATION.md exists and describes how to verify AI fallback.
- 10-CONTEXT.md points to 10-VERIFICATION.md for verification.
- Footer and AI tests were run and outcome recorded.
</verification>

<success_criteria>
- Verification steps are documented and linked from the context doc.
- Test run was performed and result documented.
</success_criteria>

<output>
After completion, create `.planning/phases/10-ai-fallback-fixes/10-02-SUMMARY.md` per template. Include a line such as: “Tests run: pytest tests/test_footer_extractor.py tests/test_ai_client.py — [PASS/FAIL].”
</output>
