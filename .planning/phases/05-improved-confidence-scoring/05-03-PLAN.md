---
phase: 05-improved-confidence-scoring
plan: 03
type: execute
wave: 3
depends_on: ["05-02"]
files_modified: [src/cli/main.py, src/pipeline/confidence_calibration.py]
autonomous: true

must_haves:
  truths:
    - "System validates confidence calibration regularly against ground truth data"
    - "CLI command validates calibration: python -m src.cli.main --validate-confidence"
    - "System reports calibration drift and suggests recalibration if drift > 5%"
    - "System can train calibration model from ground truth data"
  artifacts:
    - path: "src/cli/main.py"
      provides: "CLI command for confidence validation"
      exports: []
  key_links:
    - from: "src/cli/main.py"
      to: "src/pipeline/confidence_calibration.py"
      via: "Uses calibration functions for validation"
      pattern: "train_calibration_model|calibrate_confidence"
---

<objective>
Implement CLI command for confidence calibration validation and training.

Purpose: Add `--validate-confidence` CLI command that validates calibration against ground truth data, reports drift, and can train new calibration models. Enables regular validation workflow (CONF-05).

Output: CLI validation command, calibration training from ground truth data.
</objective>

<execution_context>
@./.claude/get-shit-done/workflows/execute-plan.md
@./.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/05-improved-confidence-scoring/05-CONTEXT.md
@src/cli/main.py
@src/pipeline/confidence_calibration.py
</context>

<tasks>

<task type="auto">
  <name>Task 1: Add calibration validation CLI command</name>
  <files>src/cli/main.py</files>
  <action>
Add `--validate-confidence` CLI command to validate calibration against ground truth data.

**Command:**
```bash
python -m src.cli.main --validate-confidence [--ground-truth PATH] [--train]
```

**Arguments:**
- `--ground-truth PATH`: Path to ground truth data file (JSON or CSV)
  - Format: `[{"invoice_id": str, "raw_confidence": float, "actual_correct": bool}, ...]`
  - If not provided, look for `data/ground_truth.json` or read from learning database (Phase 7)
- `--train`: Train new calibration model from ground truth data

**Validation Process:**
1. Load ground truth data (from file or learning database)
2. Load current calibration model (if exists)
3. For each (raw_confidence, actual_correct) pair:
   - Apply current calibration: `calibrated = calibrate_confidence(raw_confidence, model)`
   - Compare calibrated confidence to actual accuracy
4. Calculate calibration drift:
   - Group by confidence bins (0.0-0.1, 0.1-0.2, ..., 0.9-1.0)
   - For each bin, calculate: `predicted_accuracy = mean(calibrated_scores)`, `actual_accuracy = mean(actual_correct)`
   - Drift = `abs(predicted_accuracy - actual_accuracy)`
5. Report:
   - Overall calibration accuracy
   - Per-bin drift
   - Suggest recalibration if max drift > 5%

**Training Process (if --train):**
1. Load ground truth data
2. Extract (raw_scores, actual_correct) pairs
3. Train new model: `model = train_calibration_model(raw_scores, actual_correct)`
4. Save model to default path: `configs/calibration_model.pkl`
5. Report training metrics (accuracy, calibration curve)

**Output:**
- Print validation report to stdout
- If drift > 5%, suggest running with `--train` to recalibrate

Use argparse to add command. Integrate with existing CLI structure.
  </action>
  <verify>python -m src.cli.main --validate-confidence --help</verify>
  <done>--validate-confidence CLI command validates calibration, reports drift, and can train new models with --train flag</done>
</task>

<task type="auto">
  <name>Task 2: Add ground truth data format support</name>
  <files>src/pipeline/confidence_calibration.py</files>
  <action>
Add helper functions to load and process ground truth data for calibration.

**Functions to add:**

1. `load_ground_truth_data(path: str) -> Tuple[List[float], List[bool]]`:
   - Load ground truth from JSON or CSV file
   - Returns (raw_scores, actual_correct) tuples
   - Supports formats:
     - JSON: `[{"raw_confidence": 0.95, "actual_correct": true}, ...]`
     - CSV: `raw_confidence,actual_correct` with header

2. `validate_calibration(model: CalibrationModel, raw_scores: List[float], actual_correct: List[bool]) -> Dict[str, Any]`:
   - Validate calibration model against ground truth
   - Returns validation report dict:
     - `overall_accuracy: float`
     - `per_bin_drift: Dict[str, float]` (bins: "0.0-0.1", "0.1-0.2", etc.)
     - `max_drift: float`
     - `suggest_recalibration: bool` (True if max_drift > 0.05)

3. `format_validation_report(report: Dict[str, Any]) -> str`:
   - Format validation report as human-readable string
   - Include per-bin details, overall accuracy, recommendations

**Error Handling:**
- Handle missing files gracefully
- Validate data format (must have raw_confidence and actual_correct)
- Handle invalid confidence scores (must be 0.0-1.0)

Add type hints and docstrings.
  </action>
  <verify>python -c "from src.pipeline.confidence_calibration import load_ground_truth_data, validate_calibration; print('Functions imported')"</verify>
  <done>Ground truth data loading and validation functions added to confidence_calibration.py</done>
</task>

</tasks>
