---
phase: 05-improved-confidence-scoring
plan: 02
type: execute
wave: 2
depends_on: ["05-01"]
files_modified: [src/pipeline/confidence_calibration.py, src/models/invoice_header.py, src/cli/main.py]
autonomous: true

must_haves:
  truths:
    - "System calibrates confidence scores against actual accuracy (confidence 0.95 = 95% correct)"
    - "System uses isotonic regression for calibration (monotonic mapping)"
    - "System stores calibration model for reuse"
    - "System applies calibration to all confidence scores"
  artifacts:
    - path: "src/pipeline/confidence_calibration.py"
      provides: "Confidence score calibration using isotonic regression"
      exports: ["CalibrationModel", "calibrate_confidence", "train_calibration_model"]
  key_links:
    - from: "src/pipeline/confidence_calibration.py"
      to: "src/pipeline/confidence_scoring.py"
      via: "Calibrates scores from scoring function"
      pattern: "score_total_amount_candidate"
    - from: "src/pipeline/confidence_calibration.py"
      to: "src/models/invoice_header.py"
      via: "Applies calibration to total_confidence"
      pattern: "InvoiceHeader|total_confidence"
---

<objective>
Implement confidence score calibration system using isotonic regression to map raw scores to calibrated scores that reflect actual accuracy.

Purpose: Calibrate confidence scores so that 0.95 confidence = 95% correct in validation. Use isotonic regression (scikit-learn) for monotonic calibration. Store model for reuse and apply to all confidence scores.

Output: Calibration module, calibration model storage, integration with confidence scoring.
</objective>

<execution_context>
@./.claude/get-shit-done/workflows/execute-plan.md
@./.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/05-improved-confidence-scoring/05-CONTEXT.md
@.planning/research/SUMMARY.md
@.planning/research/STACK.md
@src/pipeline/confidence_scoring.py
</context>

<tasks>

<task type="auto">
  <name>Task 1: Create confidence calibration module</name>
  <files>src/pipeline/confidence_calibration.py</files>
  <action>
Create new module `src/pipeline/confidence_calibration.py` for confidence score calibration.

**Dependencies:**
- `scikit-learn>=1.3.0` (add to pyproject.toml if not present)
- Use `IsotonicRegression` from `sklearn.isotonic`

**Classes/Functions to implement:**

1. `CalibrationModel` class:
   - Stores trained isotonic regression model
   - Methods:
     - `calibrate(raw_score: float) -> float`: Apply calibration to raw score
     - `save(path: str) -> None`: Save model to file (pickle or joblib)
     - `load(path: str) -> CalibrationModel`: Load model from file (class method)

2. `train_calibration_model(raw_scores: List[float], actual_correct: List[bool]) -> CalibrationModel`:
   - Train isotonic regression on (raw_score, actual_correct) pairs
   - Returns trained CalibrationModel
   - Parameters:
     - `raw_scores`: List of raw confidence scores (0.0-1.0)
     - `actual_correct`: List of bools (True if prediction was correct, False otherwise)

3. `calibrate_confidence(raw_score: float, model: Optional[CalibrationModel] = None) -> float`:
   - Apply calibration to raw score
   - If model is None, return raw_score (no calibration)
   - Returns calibrated score (0.0-1.0)

**Calibration Process:**
- Use `IsotonicRegression(out_of_bounds='clip')` to ensure scores stay in [0, 1]
- Fit on (raw_scores, actual_correct) where actual_correct is 1.0 for True, 0.0 for False
- Isotonic regression ensures monotonic mapping (higher raw = higher calibrated)

**Model Storage:**
- Default path: `configs/calibration_model.pkl` (or `.joblib`)
- Use `joblib` for model serialization (more efficient than pickle for sklearn models)

Add type hints and docstrings.
  </action>
  <verify>python -c "from src.pipeline.confidence_calibration import CalibrationModel, train_calibration_model, calibrate_confidence; print('Module imported successfully')"</verify>
  <done>confidence_calibration.py created with CalibrationModel class, train_calibration_model, and calibrate_confidence functions</done>
</task>

<task type="auto">
  <name>Task 2: Integrate calibration into confidence scoring</name>
  <files>src/pipeline/footer_extractor.py, src/models/invoice_header.py</files>
  <action>
Integrate calibration into total amount extraction pipeline.

**Update `extract_total_amount()` in `footer_extractor.py`:**
- After scoring candidates, apply calibration if model exists
- Load calibration model from default path (`configs/calibration_model.pkl`)
- Apply `calibrate_confidence()` to all candidate scores
- Apply calibration to final `total_confidence` before storing

**Calibration Model Loading:**
- Try to load from `configs/calibration_model.pkl` (or `.joblib`)
- If file doesn't exist, skip calibration (use raw scores)
- Log warning if calibration file not found (but don't fail)

**Update InvoiceHeader:**
- No changes needed (calibration applied before storing total_confidence)

**Configuration:**
- Add optional config option: `calibration.enabled: bool = True`
- Add optional config option: `calibration.model_path: str = "configs/calibration_model.pkl"`

**Error Handling:**
- If calibration model fails to load, fall back to raw scores
- Log error but don't fail extraction

Update imports and ensure calibration is optional (works without model).
  </action>
  <verify>python -c "from src.pipeline.footer_extractor import extract_total_amount; print('Function imports calibration')"</verify>
  <done>extract_total_amount applies calibration to confidence scores if model exists, falls back to raw scores if model not found</done>
</task>

</tasks>
